{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import MultivariateNormal\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc_mean = nn.Linear(dim, dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "        # Value function baseline\n",
    "        self.value_net = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.fc_mean(x)\n",
    "        cov = torch.diag(torch.exp(self.log_std))\n",
    "        return MultivariateNormal(mean, cov), self.value_net(x)\n",
    "\n",
    "    \n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def compute_reward(v_new, v_prev, A, lambda_):\n",
    "    \"\"\"Reward function using Rayleigh quotient\"\"\"\n",
    "    # Compute Rayleigh quotient\n",
    "    with torch.no_grad():\n",
    "        R = (v_new @ A @ v_new) / (v_new @ v_new)\n",
    "    \n",
    "    # 1. Rayleigh quotient component (closer to true eigenvalue is better)\n",
    "    rayleigh_component = -torch.abs(R - lambda_).item()\n",
    "    \n",
    "    # 2. Direction consistency component\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "        v_new.unsqueeze(0), v_prev.unsqueeze(0)\n",
    "    ).item()\n",
    "    direction_component = 0.3 * cosine_sim\n",
    "    \n",
    "    # 3. Improvement component (compared to previous Rayleigh quotient)\n",
    "    with torch.no_grad():\n",
    "        prev_R = (v_prev @ A @ v_prev) / (v_prev @ v_prev)\n",
    "    improvement_component = 0.2 * (torch.abs(prev_R - lambda_).item() - torch.abs(R - lambda_).item())\n",
    "    \n",
    "    # Combined reward\n",
    "    reward = rayleigh_component + direction_component + improvement_component\n",
    "    \n",
    "    # Also compute residual for tracking\n",
    "    residual = torch.linalg.norm((A - lambda_ * torch.eye(A.shape[0])) @ v_new)\n",
    "    \n",
    "    return reward, residual.item(), cosine_sim\n",
    "\n",
    "def train(A, lambda_, policy, epochs=500, batch_size=32, replay_size=1000, dominant_v=None):\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    replay_buffer = ExperienceReplay(replay_size)\n",
    "    v = torch.randn(A.shape[0])\n",
    "    v = v / torch.norm(v)\n",
    "    \n",
    "    # Tracking variables\n",
    "    best_v = v.clone()\n",
    "    best_residual = float('inf')\n",
    "    moving_avg_reward = 0\n",
    "    alpha = 0.1  # For moving average\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Generate multiple trajectories for batch update\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            v_prev = v.clone()\n",
    "            dist, value_est = policy(v_prev)\n",
    "            delta_v = dist.sample()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                v_new = v_prev + delta_v\n",
    "                v_new = v_new / torch.norm(v_new)\n",
    "                reward, residual, cos_sim = compute_reward(v_new, v_prev, A, lambda_)\n",
    "                \n",
    "                # Update best solution found\n",
    "                if residual < best_residual:\n",
    "                    best_residual = residual\n",
    "                    best_v = v_new.clone()\n",
    "                \n",
    "                # Update moving average for baseline\n",
    "                moving_avg_reward = alpha * reward + (1 - alpha) * moving_avg_reward\n",
    "            \n",
    "            # Store experience\n",
    "            replay_buffer.push((v_prev, delta_v, reward, value_est))\n",
    "            \n",
    "            # Store for batch update\n",
    "            states.append(v_prev)\n",
    "            actions.append(delta_v)\n",
    "            rewards.append(reward)\n",
    "            values.append(value_est)\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            replay_batch = replay_buffer.sample(batch_size)\n",
    "            replay_states, replay_actions, replay_rewards, replay_values = zip(*replay_batch)\n",
    "            \n",
    "            # Combine with current batch\n",
    "            states.extend(replay_states)\n",
    "            actions.extend(replay_actions)\n",
    "            rewards.extend(replay_rewards)\n",
    "            values.extend(replay_values)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        old_values = torch.cat(values).squeeze()\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = rewards - old_values.detach()\n",
    "        \n",
    "        # Policy loss\n",
    "        dists, value_ests = policy(states)\n",
    "        log_probs = dists.log_prob(actions)\n",
    "        \n",
    "        # PPO-style clipped objective\n",
    "        ratios = torch.exp(log_probs - dists.log_prob(actions).detach())\n",
    "        clipped_ratios = torch.clamp(ratios, 0.8, 1.2)\n",
    "        policy_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "        \n",
    "        # Value loss (MSE)\n",
    "        value_loss = 0.5 * (value_ests.squeeze() - rewards).pow(2).mean()\n",
    "        \n",
    "        # Entropy bonus\n",
    "        entropy = dists.entropy().mean()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update v with best candidate from batch\n",
    "        v = best_v.clone()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(\n",
    "                f'Epoch {epoch}: Loss = {loss.item():.4f}, '\n",
    "                f'Residual = {best_residual:.4f}, '\n",
    "                f'Avg Reward = {moving_avg_reward:.4f}'\n",
    "                f', Cosine Similarity = {torch.nn.functional.cosine_similarity(best_v.unsqueeze(0), dominant_v.unsqueeze(0)).item():.4f}'\n",
    "                )\n",
    "    \n",
    "    return best_v\n",
    "\n",
    "# Test function with improved training\n",
    "def test_improved_policy(dim=5, epochs=1000):\n",
    "    # Generate random symmetric matrix\n",
    "    A_np = np.random.randn(dim, dim)\n",
    "    A_np = A_np + A_np.T\n",
    "    A = torch.tensor(A_np, dtype=torch.float32)\n",
    "    \n",
    "    # Get ground truth\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A_np)\n",
    "    dominant_idx = np.argmax(np.abs(eigenvalues))\n",
    "    dominant_lambda = eigenvalues[dominant_idx]\n",
    "    dominant_v = torch.tensor(eigenvectors[:, dominant_idx], dtype=torch.float32)\n",
    "    \n",
    "    print(f\"True dominant eigenvalue: {dominant_lambda}\")\n",
    "    \n",
    "    # Train\n",
    "    policy = Policy(dim)\n",
    "    predicted_v = train(A, dominant_lambda, policy, epochs=epochs, dominant_v=dominant_v)\n",
    "    \n",
    "    # Compare\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "        predicted_v.unsqueeze(0), dominant_v.unsqueeze(0)\n",
    "    ).item()\n",
    "    \n",
    "    print(f\"\\nPredicted eigenvector: {predicted_v.detach().numpy()}\")\n",
    "    print(f\"True eigenvector: {dominant_v.numpy()}\")\n",
    "    print(f\"Cosine similarity: {cosine_sim:.4f}\")\n",
    "\n",
    "    return cosine_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True dominant eigenvalue: 28.5666090409687\n",
      "Epoch 0: Loss = -1.2575, Residual = 29.1165, Avg Reward = -27.1810, Cosine Similarity = -0.0459\n",
      "Epoch 100: Loss = -1.1973, Residual = 24.7676, Avg Reward = -29.7029, Cosine Similarity = -0.1759\n",
      "Epoch 200: Loss = -1.1741, Residual = 24.7676, Avg Reward = -27.9813, Cosine Similarity = -0.1759\n",
      "Epoch 300: Loss = -1.0621, Residual = 23.1141, Avg Reward = -28.7234, Cosine Similarity = -0.1049\n",
      "Epoch 400: Loss = -1.1233, Residual = 22.5609, Avg Reward = -28.0097, Cosine Similarity = 0.1651\n",
      "Epoch 500: Loss = -1.1775, Residual = 20.2576, Avg Reward = -23.2506, Cosine Similarity = 0.0544\n",
      "Epoch 600: Loss = -1.0616, Residual = 16.8059, Avg Reward = -18.8056, Cosine Similarity = -0.0240\n",
      "Epoch 700: Loss = -1.1173, Residual = 14.5934, Avg Reward = -13.4332, Cosine Similarity = 0.0886\n",
      "Epoch 800: Loss = -1.1135, Residual = 12.1384, Avg Reward = -11.2219, Cosine Similarity = 0.1100\n",
      "Epoch 900: Loss = -1.1148, Residual = 10.1986, Avg Reward = -9.5039, Cosine Similarity = 0.1166\n",
      "Epoch 1000: Loss = -1.0788, Residual = 9.9863, Avg Reward = -7.8633, Cosine Similarity = 0.1624\n",
      "Epoch 1100: Loss = -1.0382, Residual = 9.2305, Avg Reward = -6.9807, Cosine Similarity = 0.2285\n",
      "Epoch 1200: Loss = -1.0082, Residual = 8.3571, Avg Reward = -6.0535, Cosine Similarity = 0.1881\n",
      "Epoch 1300: Loss = -0.9993, Residual = 7.7692, Avg Reward = -5.5153, Cosine Similarity = 0.2251\n",
      "Epoch 1400: Loss = -0.9807, Residual = 6.9790, Avg Reward = -5.0910, Cosine Similarity = 0.2617\n",
      "\n",
      "Predicted eigenvector: [ 0.12241468  0.04314192  0.06642442 -0.16373478  0.15400186  0.12988366\n",
      " -0.08964941  0.21900861 -0.11121306 -0.12559359 -0.08720329  0.02870207\n",
      "  0.04425667  0.0913056   0.15065745 -0.0576343   0.01712527  0.07980905\n",
      "  0.12570898 -0.01198104  0.04229657  0.10073654 -0.00936944  0.11699489\n",
      " -0.13543981  0.17606279 -0.09781678  0.03481677  0.03442004  0.05953106\n",
      " -0.05811383 -0.03752695 -0.04477998 -0.06265855 -0.01167327  0.01479289\n",
      "  0.03932198  0.0609799  -0.07411776  0.11253161 -0.00660867 -0.0192778\n",
      " -0.16126618 -0.04589796 -0.02366468  0.1654293   0.09587231 -0.03615265\n",
      "  0.01128906  0.04533162  0.04736369  0.06307538  0.03467316 -0.01088137\n",
      " -0.04678473  0.06121628  0.05242456  0.15545188  0.01227307 -0.06279037\n",
      "  0.07537113 -0.09444696  0.1030636  -0.07622284 -0.1345677  -0.09106944\n",
      "  0.1807706  -0.04718168 -0.15888102 -0.16864073 -0.07846132 -0.04336103\n",
      " -0.26271406  0.11886152  0.11566571  0.04717727 -0.01116301  0.09224673\n",
      "  0.03452498  0.1694218  -0.16751686  0.15294723  0.03465711 -0.12847218\n",
      " -0.07343511 -0.05702287  0.19239642 -0.11134712  0.04834811 -0.02550051\n",
      "  0.14163342 -0.04309084  0.00150957  0.09136432 -0.00764719 -0.07720179\n",
      " -0.03197847  0.03536073  0.21644239  0.11924084]\n",
      "True eigenvector: [-0.21462272  0.09680893 -0.02244322 -0.03621921  0.13638413  0.02531323\n",
      " -0.16881049  0.02123333 -0.124749   -0.23724373  0.00630121  0.11031269\n",
      " -0.10982376  0.00366175  0.18869258 -0.09692133  0.05575249 -0.05446103\n",
      "  0.17416225  0.07983644  0.08061757 -0.03162092 -0.09157742  0.03851781\n",
      "  0.06582604  0.1502352  -0.07234164  0.05762096  0.11482494  0.15836564\n",
      " -0.14467111 -0.07488853  0.10842928 -0.08230506 -0.00446514 -0.01162786\n",
      "  0.06104416 -0.10675733  0.02206797  0.0495533  -0.0306444   0.04633245\n",
      " -0.02586172 -0.08190233  0.02000707 -0.03967228 -0.0869265   0.07451486\n",
      " -0.08157719  0.0724946   0.09764532  0.02611529 -0.103252    0.11350938\n",
      " -0.02002246 -0.10921258  0.01677929  0.05906664  0.2619708  -0.11093541\n",
      "  0.12547667  0.04642494 -0.00139936  0.02857986  0.00824523 -0.05577805\n",
      "  0.01928817 -0.11161643  0.03790862  0.06788188 -0.0369661   0.13062504\n",
      " -0.2087732  -0.14293154 -0.19696704  0.02925049 -0.06559921  0.10502709\n",
      "  0.13414949  0.04042098  0.03746114  0.19290334 -0.0373395  -0.10428502\n",
      "  0.04662326  0.06239702 -0.06187145 -0.02272478  0.09075772  0.11801827\n",
      "  0.12160608 -0.07317404 -0.03523042  0.04953485  0.19844754 -0.01502019\n",
      " -0.00736717 -0.19962212  0.0357609   0.04578912]\n",
      "Cosine similarity: 0.2689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26886534690856934"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_improved_policy(dim=100, epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [5, 10, 20, 50, 75, 100]:\n",
    "    print(f\"\\nTesting with dimension: {dim}\")\n",
    "    for i in range(3):\n",
    "        print(f\"Run {i + 1}\")\n",
    "        test_improved_policy(dim=dim, epochs=1001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [5, 10, 20, 50, 75, 100]\n",
    "results = []\n",
    "for dim in choices:\n",
    "    print(f\"\\nTesting with dimension: {dim}\")\n",
    "    for i in range(10):\n",
    "        print(f\"Run {i + 1}\")\n",
    "        x = test_improved_policy(dim=dim, epochs=1001)\n",
    "        results.append(x)\n",
    "results = np.array(results)\n",
    "results = results.reshape(len(choices), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with dimension: 5\n",
      "Run 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_improved_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         x = \u001b[43mtest_improved_policy\u001b[49m(dim=dim, epochs=\u001b[32m1001\u001b[39m)\n\u001b[32m      8\u001b[39m         results.append(x)\n\u001b[32m      9\u001b[39m results = np.array(results)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_improved_policy' is not defined"
     ]
    }
   ],
   "source": [
    "choices = [5, 10]\n",
    "results = []\n",
    "for dim in choices:\n",
    "    print(f\"\\nTesting with dimension: {dim}\")\n",
    "    for i in range(3):\n",
    "        print(f\"Run {i + 1}\")\n",
    "        x = test_improved_policy(dim=dim, epochs=1001)\n",
    "        results.append(x)\n",
    "results = np.array(results)\n",
    "results = results.reshape(len(choices), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
